{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64660143-440f-42c2-95dd-2828345a504b",
   "metadata": {},
   "source": [
    "# Introduction to MPI (distributed memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d299b88",
   "metadata": {},
   "source": [
    "\n",
    "## Intro MPI\n",
    "\n",
    "MPI stands for Message Passing Interface. It is a standard which allows\n",
    "processes to communicate and share data across an heterogeneous setup.\n",
    "MPI is well suited for distributed memory systems, although it can also\n",
    "be used in multiprocessor environments (on those environments, OpenMP is\n",
    "a better suited solution). Both MPI (on any of its implementations) and\n",
    "OpenMP represent the most common approaches to program on a cluster\n",
    "environment.\n",
    "\n",
    "> NOTE: This section is heavily based on Chapter 13 of \"High Performance\n",
    "> Linux Clusters with Oscar, Rocks, OpenMosix, and MPI\", J. D. Sloan,\n",
    "> O'Reilly.\n",
    "\n",
    "MPI is a library of routines which allows the parts of your parallel\n",
    "program to communicate. But it is up to you how to split up your\n",
    "program/problem. There are no typical restrictions on the number of\n",
    "processors you can split on your problem: you can use less or more\n",
    "process than the number of available processors. For that reason, MPI\n",
    "allows you to tag every process with a given *rank*, on a problem of\n",
    "*size* total number of processes.\n",
    "\n",
    "MPI can be used as a library on several languages: Fortran, C, C++ ,\n",
    "etc. In the following, we will use mostly C.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71527542",
   "metadata": {},
   "source": [
    "\n",
    "## Core MPI\n",
    "\n",
    "It is typical to denote the rank-0 process as the master and the\n",
    "remaining process (rank \\>= 1) as slaves. Most of the time, the slaves\n",
    "are processing several parts of the problem and then communicate back\n",
    "the results to the master. All of this is managed on a single source\n",
    "code which is advantageous. The following is an example of the most\n",
    "fundamental MPI program which constitutes a template for more complex\n",
    "programs:\n",
    "\n",
    "``` cpp\n",
    "#include \"mpi.h\"\n",
    "#include <cstdio>\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  int processId;                 /* rank of process */\n",
    "  int noProcesses;               /* number of processes */\n",
    "\n",
    "  MPI_Init(&argc, &argv);                   /* Mandatory */\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &noProcesses);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &processId);\n",
    "\n",
    "  std::fprintf(stdout, \"Hello from process %d of %d\\n\", processId, noProcesses);\n",
    "\n",
    "  MPI_Finalize();                       /* Mandatory */\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "Before compiling and running, we will explain the code. First of all,\n",
    "the MPI functions are added by the inclusion of the `mpi.h` header. The\n",
    "two mandatory functions `MPI_Init` and `MPI_Finalize`, must be always\n",
    "present. They set-up and destroy the MPI running environment. The other\n",
    "two, `MPI_Comm_size` and `MPI_Comm_rank` allows to identify the size of\n",
    "the problem (number of processes) and the rank of the current process.\n",
    "This allows to distinguish the master from the slaves and to split the\n",
    "problem.\n",
    "\n",
    "MPI<sub>Init</sub>  \n",
    "This function call **must** occur before any other MPI function call.\n",
    "This function initializes the MPI session. It receives the argc and argv\n",
    "arguments, as shown, which allows the program to read command line\n",
    "arguments.\n",
    "\n",
    "MPI<sub>Finalize</sub>  \n",
    "This function releases the MPI environment. This should be the last call\n",
    "to MPI in the program, after all communications have ended. Your code\n",
    "must be written in such a way that all processes call this function.\n",
    "\n",
    "MPI<sub>Commsize</sub>  \n",
    "This function returns the total number of processes running on a\n",
    "communicator. A communicator is the communication group used by the\n",
    "processes. The default communicator is `MPI_COMM_WORLD`. You can create\n",
    "your own communicators to isolate several sub-groups of processes.\n",
    "\n",
    "MPI<sub>Commrank</sub>  \n",
    "Returns the actual rank of the calling process, inside the communicator\n",
    "group. The range goes from 0 to the the size minus 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14c58c",
   "metadata": {},
   "source": [
    "\n",
    "### MPI<sub>Getprocessorname</sub>\n",
    "\n",
    "This retrieves the hostname of the current and is very useful for\n",
    "debugging purposes. The following example shows how to get the node name\n",
    "\n",
    "``` cpp\n",
    "#include \"mpi.h\"\n",
    "#include <cstdio>\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  int processId;                 /* rank of process */\n",
    "  int noProcesses;               /* number of processes */\n",
    "  int nameSize;              /* lenght of name */\n",
    "  char computerName[MPI_MAX_PROCESSOR_NAME];\n",
    "\n",
    "  MPI_Init(&argc, &argv);                   /* Mandatory */\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &noProcesses);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &processId);\n",
    "\n",
    "  MPI_Get_processor_name(computerName, &nameSize);\n",
    "  fprintf(stderr, \"Hello from process %d of %d, on %s\\n\", processId, noProcesses, computerName);\n",
    "\n",
    "  MPI_Finalize();                       /* Mandatory */\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa87eb",
   "metadata": {},
   "source": [
    "\n",
    "### Compiling and running the MPI program\n",
    "\n",
    "Compilation can be done directly with the gcc compiler by using the\n",
    "appropriate flags. But there is a helper command ready to help you:\n",
    "`mpicc`. Save the previous example code into a file (here called\n",
    "`mpi-V1.c`) and run the following command\n",
    "\n",
    "``` bash\n",
    "$ mpicc mpi-V1.c -o hello.x\n",
    "```\n",
    "\n",
    "If you run the command as usual, it will expand to the number of local\n",
    "cores and run locally only (for example, ina two cores system):\n",
    "\n",
    "``` bash\n",
    "$ ./hello.x\n",
    "Hello from process 0 of 2, on iluvatarMacBookPro.local\n",
    "Hello from process 1 of 2, on iluvatarMacBookPro.local\n",
    "```\n",
    "\n",
    "But, there is another utility which allows you to specify several\n",
    "parameters, like the number of processes to span, among others:\n",
    "\n",
    "``` bash\n",
    "$ mpirun -np 5 ./hello.x\n",
    "Hello from process 3 of 5, on iluvatarMacBookPro.local\n",
    "Hello from process 1 of 5, on iluvatarMacBookPro.local\n",
    "Hello from process 4 of 5, on iluvatarMacBookPro.local\n",
    "Hello from process 0 of 5, on iluvatarMacBookPro.local\n",
    "Hello from process 2 of 5, on iluvatarMacBookPro.local\n",
    "```\n",
    "\n",
    "1.  Exercise: Print the output sorted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f385b",
   "metadata": {},
   "source": [
    "\n",
    "### Running an mpi on several machines\n",
    "\n",
    "You can run an mpi process on the same machine or in several machines,\n",
    "as follows\n",
    "\n",
    "``` bash\n",
    "$ mpirun -np 4 -host host01,host02 ./a.out\n",
    "```\n",
    "\n",
    "where `host01`, etc is the hostname where you want to run the process.\n",
    "You will need to have access by ssh to those hosts, and preferable, a\n",
    "password-less [setup](http://lmgtfy.com/?q=passwordless+ssh) . See also\n",
    "the manual for `mpirun` .\n",
    "\n",
    "You can specify a machine file for mpi tu use them and run the command\n",
    "with given resources. An example machine files is as follows\n",
    "\n",
    "``` bash\n",
    "hostname1 slots=2\n",
    "hostname2 slots=4\n",
    "hostname3 slots=6\n",
    "```\n",
    "\n",
    "where hostname is the ip or the hostname of a given machine and the\n",
    "slots indicates how many jobs to run. You will need to have a\n",
    "passwordless access or similar to those computers.\n",
    "\n",
    "To configure the passwordless ssh, you can follow the many tutorials\n",
    "online, which basically are the following steps\n",
    "\n",
    "-   Generate the rsa key\n",
    "\n",
    "    ``` shell\n",
    "    ssh-keygen -t rsa\n",
    "    ```\n",
    "\n",
    "    Do not set ay passphrase, just press enter.\n",
    "\n",
    "-   Copy the id to the computer where you want to run processes\n",
    "    remotely,\n",
    "\n",
    "    ``` shell\n",
    "    ssh-copy-id 192.168.10.16\n",
    "    ```\n",
    "\n",
    "    Enter your password for the last time. If everything goes ok, test\n",
    "    that you can login remotely without password by runnung\n",
    "\n",
    "        ssh 192.168.10.16\n",
    "\n",
    "Now you can run the process remotely:\n",
    "\n",
    "``` shell\n",
    "mpirun -np 100 -host 192.168.10.15,192.168.10.16 --oversubscribe ./a.out\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37802e",
   "metadata": {},
   "source": [
    "\n",
    "## Application: Numerical Integration (Send and Recv functions)\n",
    "\n",
    "To improve our understanding of MPI, and to introduce new functions, we\n",
    "will use a familiar problem to investigate: the computation of the area\n",
    "under a curve. Its easiness and familiarity allows to focus on MPI and\n",
    "its usage. Let's introduce first the serial version.\n",
    "\n",
    "### Serial implementation of the numerical integration by rectangles\n",
    "\n",
    "``` cpp\n",
    "#include <cstdio>\n",
    "\n",
    "/* Problem parameters */\n",
    "double f(double x);\n",
    "double integral_serial(double xmin, double xmax, double n);\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  const int numberRects = 50;\n",
    "  const double lowerLimit = 2.0;\n",
    "  const double upperLimit = 5.0;\n",
    "\n",
    "  double integral = integral_serial(lowerLimit, upperLimit, numberRects);\n",
    "  printf(\"The area from %lf to %lf is : %lf\\n\", lowerLimit, upperLimit, integral);\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "double f(double x) {\n",
    "  return x*x;\n",
    "}\n",
    "\n",
    "double integral_serial(double xmin, double xmax, double n)\n",
    "{\n",
    "  double area = 0.0;\n",
    "  double width = (xmax - xmin)/n;\n",
    "\n",
    "  for (int i = 0; i < n; ++i) {\n",
    "    double at = xmin + i*width + width/2.0; // center in the middle\n",
    "    double heigth = f(at);\n",
    "    area = area + width*heigth;\n",
    "  }\n",
    "  return area;\n",
    "}\n",
    "```\n",
    "\n",
    "Compile and run it :\n",
    "\n",
    "``` shell\n",
    "$ g++ serial_integral.cpp\n",
    "$ ./a.out\n",
    "The area from 2.000000 to 5.000000 is : 38.999100\n",
    "```\n",
    "\n",
    "Now let's implement an mpi solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9fb27",
   "metadata": {},
   "source": [
    "\n",
    "### MPI solution: Point to point communication with MPI<sub>Send</sub> and MPI<sub>Recv</sub>\n",
    "\n",
    "Check: <https://mpitutorial.com/tutorials/mpi-send-and-receive/>\n",
    "\n",
    "This is an embarrassingly parallel problem. There are no data\n",
    "dependencies. We can partition the domain, performing partial sums per\n",
    "process, and then cumulate all of them. Basically, we will use\n",
    "`MPI_Comm_rank` and `MPI_Comm_size` to partition the problem, and we\n",
    "will introduce two new functions: `MPI_Send`, to send data, and\n",
    "`MPI_Recv`, to receive data. These are known as blocking, point-to-point\n",
    "communications.\n",
    "\n",
    "Before checking the code, please **work in groups** to deduce the right\n",
    "problem partition (keeping the same number of inteervals per process)\n",
    "and how to communicate data. Use a shared whiteboard, like google\n",
    "jamboard.\n",
    "\n",
    "``` cpp\n",
    "#include \"mpi.h\"\n",
    "#include <cstdio>\n",
    "#include <cstdlib>\n",
    "\n",
    "/* Problem parameters */\n",
    "double f(double x);\n",
    "double integral_serial(double xmin, double xmax, double n);\n",
    "void integral_mpi(double xmin, double xmax, double n, int pid, int np);\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  const int numberRects = std::atoi(argv[1]);\n",
    "  const double lowerLimit = 2.0;\n",
    "  const double upperLimit = 5.0;\n",
    "\n",
    "  /* MPI Variables */\n",
    "\n",
    "  /* problem variables */\n",
    "  int pid, np;\n",
    "\n",
    "  /* MPI setup */\n",
    "  MPI_Init(&argc, &argv);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
    "\n",
    "  integral_mpi(lowerLimit, upperLimit, numberRects, pid, np);\n",
    "\n",
    "  /* finish */\n",
    "  MPI_Finalize();\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "double integral_serial(double xmin, double xmax, double n)\n",
    "{\n",
    "  double area = 0.0;\n",
    "  double width = (xmax - xmin)/n;\n",
    "\n",
    "  for (int i = 0; i < n; ++i) {\n",
    "    double at = xmin + i*width + width/2.0; // center in the middle\n",
    "    double heigth = f(at);\n",
    "    area = area + width*heigth;\n",
    "  }\n",
    "  return area;\n",
    "}\n",
    "\n",
    "void integral_mpi(double xmin, double xmax, double n, int pid, int np)\n",
    "{\n",
    "  /* Adjust problem size for sub-process */\n",
    "  double range = (xmax - xmin) / np;\n",
    "  double lower = xmin + range*pid;\n",
    "\n",
    "  /* Calculate area for subproblem */\n",
    "  double area = integral_serial(lower, lower+range, n);\n",
    "\n",
    "  /* Collect info and print results */\n",
    "  MPI_Status status;\n",
    "  int tag = 0;\n",
    "  if (0 == pid) { /* Master */\n",
    "    double total = area;\n",
    "    for (int src = 1; src < np; ++src) {\n",
    "      MPI_Recv(&area, 1, MPI_DOUBLE, src, tag, MPI_COMM_WORLD, &status);\n",
    "      total += area;\n",
    "    }\n",
    "    fprintf(stderr, \"The area from %g to %g is : %25.16e\\n\", xmin, xmax, total);\n",
    "  }\n",
    "  else { /* slaves only send */\n",
    "    int dest = 0;\n",
    "    MPI_Send(&area, 1, MPI_DOUBLE, dest, tag, MPI_COMM_WORLD);\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "double f(double x) {\n",
    "  return x*x;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Compile as usual,\n",
    "\n",
    "``` shell\n",
    "mpic++ mpi_integral.cpp\n",
    "```\n",
    "\n",
    "and run it\n",
    "\n",
    "``` shell\n",
    "$ mpirun -np 2 ./a.out\n",
    "$ mpirun -np 10 ./a.out\n",
    "```\n",
    "\n",
    "> Exercise: Play with the number of processes. Are you getting a better\n",
    "> answer for increasing number of processes?\n",
    "\n",
    "In this example we have kept constant the number of rectangles per\n",
    "range. That means that as the number of processess increases, the total\n",
    "number of rectangles increases too, since each process has the same\n",
    "original number of rectangles. Therefore, the more the processes, the\n",
    "better the precision. If you prefer speed over precision, you will have\n",
    "to fix the total number of rectangles, whose number per processor will\n",
    "decrease.\n",
    "\n",
    "> **Exercise** : Change the previous code to handle a constant number of\n",
    "> rectangles. As you increase the number of processors, is the precision\n",
    "> increasing? or the speed?\n",
    "\n",
    "The important part here is the scaling of the problem to the actual\n",
    "number of processes. For example, for five processes, we need first to\n",
    "divide the total interval (from 2 to 5 in tis example) into the number\n",
    "of processes, and then take each one of this sub-intervals and process\n",
    "it on the given process (identified by processId). This is called a\n",
    "domain-decomposition approach.\n",
    "\n",
    "At the end of the code, we verify if we are on a slave or on the master.\n",
    "If we are the master, we receive all the partial integrations sent by\n",
    "the slaves, and then we print the total. If we are a slave, we just send\n",
    "our result back to the master (destination = 0). Using these functions\n",
    "is simple but not always efficient since, for example, for 100\n",
    "processes, we are calling `MPI_Send` and `MPI_Recv` 99 times.\n",
    "\n",
    "`MPI_Send`  \n",
    "used to send information to one process to another. A call to `MPI_Send`\n",
    "*MUST* be matched with a corresponding `MPI_Recv` . Information is typed\n",
    "(the type is explicitly stated), and tagged. The first argument of\n",
    "`MPI_Send` gives the starting address of the data to be transmitted. The\n",
    "second is the number of items to be sent, and the third one is the data\n",
    "type (`MPI_DOUBLE`). There are more datatypes like `MPI_BYTE`,\n",
    "`MPI_CHAR`, `MPI_UNSIGNED`, `MPI_INT`, `MPI_FLOAT`, `MPI_PACKED`, etc.\n",
    "The next argument is the destination, that is, the rank of the receiver.\n",
    "The next one is a tag, which is used for buffer purposes. Finally, the\n",
    "(default) communicator is used.\n",
    "\n",
    "`MPI_Recv`  \n",
    "allows to receive information sent by `MPI_Send`. Its first three\n",
    "arguments are as before. Then it comes the destination (rank of the\n",
    "receiving process), the tag, the communicator, and a status flag.\n",
    "\n",
    "Both `MPI_Send` and `MPI_Recv` are blocking calls. If a process is\n",
    "receiving information which has not been sent yet, if waits until it\n",
    "receives that info without doing anything else.\n",
    "\n",
    "Before continue, please check the following mpi functions:\n",
    "\n",
    "-   `MPI_ISend , MPI_Irecv`\n",
    "-   `MPI_Sendrecv`\n",
    "-   `MPI_Sendrecv_replace`\n",
    "\n",
    "> **Exercise:** Discuss if any of the previous can be used for the\n",
    "> integral. Or what pattern would you think could be useful? maybe a\n",
    "> reduction?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd9e9a",
   "metadata": {},
   "source": [
    "\n",
    "## More Point to point exercises\n",
    "\n",
    "Check also:\n",
    "<http://www.archer.ac.uk/training/course-material/2017/09/mpi-york/exercises/MPP-exercises.pdf>\n",
    "\n",
    "### Ping-Pong\n",
    "\n",
    "Ref: Gavin, EPCC\n",
    "\n",
    "-   Write an MPI code for 2 processes\n",
    "-   The first task sends a message to the second task\n",
    "-   The message can be a single integer\n",
    "-   The second task receives this message\n",
    "-   The second task changes the message somehow\n",
    "    -   Add 99 to the original message, for instance\n",
    "-   The second task sends the message back to the first task\n",
    "-   The first task receives the message\n",
    "-   The first task prints this new message to the screen\n",
    "\n",
    "Maybe repeat M times\n",
    "\n",
    "### Bandwitdh local and network\n",
    "\n",
    "Compute the bandwidth when sending data locally (same computer) and to\n",
    "other computer in the network. Plot the time taken to send the data as a\n",
    "function of the buffer size. Is the bandwidth (Mbits/s) constant? is\n",
    "there any difference between local and multimachine data sending?\n",
    "\n",
    "### Large unit matrix\n",
    "\n",
    "Imagine a large unit matrix that is distributed among all the processes\n",
    "in horizontal layers. Each process stores only its corresponding matrix\n",
    "part, not the full matrix. Make each process fill its corresponding part\n",
    "and then print the full matrix in the right order in the master process\n",
    "by sending each process matrix section to the master. Also plot it in\n",
    "gnupot or matplotlib\n",
    "\n",
    "### Ring pattern\n",
    "\n",
    "Ref: Gavin, EPCC\n",
    "\n",
    "-   Write an MPI code which sends a message around a ring\n",
    "-   Each MPI task receives from the left and passes that message on to\n",
    "    the right\n",
    "-   Every MPI task sends a message: Its own rank ID\n",
    "-   How many steps are required for the message to return ‘home’.\n",
    "-   Update the code so that each MPI Tasks takes the incoming message\n",
    "    (an integer), and adds it to a running total\n",
    "-   What should the answer be on MPI Task 0 . Is the answer the same on\n",
    "    all tasks? (Test with unblocking send and even/odd send receive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0379c",
   "metadata": {},
   "source": [
    "\n",
    "## Collective Communications\n",
    "\n",
    "Check:\n",
    "<https://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/>\n",
    "\n",
    "If you check some of the previous code, or the integral code, we are\n",
    "sending the same info over and over to every other process. While the\n",
    "master sends the data to process 1, the remaining processes are idle.\n",
    "This is not efficient. Fortunately, MPI provides several functions to\n",
    "improve this situation, where the overall communication is managed much\n",
    "better. The first function is `MPI_Bcast`.\n",
    "\n",
    "`MPI_Bcast` \n",
    "provides a mechanism to distribute the same information\n",
    "\n",
    "among a communicator group, in an efficient manner. It takes five\n",
    "arguments. The first three are the data to be transmitted, as usual. The\n",
    "fourth one is the rank of the process generating the broadcast, a.k.a\n",
    "the root of the broadcast. The final argument is the communicator\n",
    "identifier. See:\n",
    "<https://www.open-mpi.org/doc/v4.1/man3/MPI_Bcast.3.php> . The syntax is\n",
    "\n",
    "``` cpp\n",
    "int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype,\n",
    "              int root, MPI_Comm comm)\n",
    "```\n",
    "\n",
    "where `root` is the one sending the data in buffer to all other\n",
    "processes.\n",
    "\n",
    "<img src=\"img/mpi_broadcast.png\" class=centerimg50 >\n",
    "\n",
    "It could be internally optimized to reduce calls\n",
    "\n",
    "<img src=\"img/mpi_broadcast_ex.png\" class=centerimg50 >\n",
    "\n",
    "`MPI_Reduce`  \n",
    "allows to collect data and, at the same time, perform a\n",
    "\n",
    "mathematical operation on it (like collecting the partial results and\n",
    "then summing up them). It requires not only a place where to put the\n",
    "results but also the operator to be applied. This could be, for example,\n",
    "`MPI_SUM`, `MPI_MIN`, `MPI_MAX`, etc. It also needs the identifier of\n",
    "the root of the broadcast. Both this functions will simplify a lot the\n",
    "code, and also make it more efficient (since the data distribution and\n",
    "gathering is optimised).\n",
    "<https://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/>\n",
    "\n",
    "<img src=\"img/mpi_reduce.png\" class=centerimg50 >\n",
    "\n",
    "Check:\n",
    "<https://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/>\n",
    "<http://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture29.pdf>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e6352",
   "metadata": {},
   "source": [
    "\n",
    "### MPI Bcast\n",
    "\n",
    "1.  Sending an array from one source to all processes\n",
    "\n",
    "    ``` cpp\n",
    "    #include \"mpi.h\"\n",
    "    #include <iostream>\n",
    "    #include <cstdlib>\n",
    "    #include <algorithm>\n",
    "    #include <numeric>\n",
    "\n",
    "    void send_data_collective(int size, int pid, int np);\n",
    "\n",
    "    int main(int argc, char **argv)\n",
    "    {\n",
    "      int np, pid;\n",
    "\n",
    "      /* MPI setup */\n",
    "      MPI_Init(&argc, &argv);\n",
    "      MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
    "      MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
    "\n",
    "      const int SIZE = std::atoi(argv[1]);\n",
    "      send_data_collective(SIZE, pid, np);\n",
    "\n",
    "      MPI_Finalize();\n",
    "\n",
    "      return 0;\n",
    "    }\n",
    "\n",
    "    void send_data_collective(int size, int pid, int np)\n",
    "    {\n",
    "      // create data buffer (all processes)\n",
    "      double * data = new double [size];\n",
    "      if (0 == pid) {\n",
    "        std::iota(data, data+size, 0.0); // llena como 0 1 2 3 4\n",
    "      }\n",
    "      // send data to all processes\n",
    "      int root = 0;\n",
    "      double start = MPI_Wtime();\n",
    "      MPI_Bcast(&data[0], size, MPI_DOUBLE, root, MPI_COMM_WORLD);\n",
    "      double end = MPI_Wtime();\n",
    "      // print size, time, bw in root\n",
    "      if (0 == pid) {\n",
    "        int datasize = sizeof(double)*size;\n",
    "        std::cout << datasize << \"\\t\" << (end-start) << \"\\t\"\n",
    "                  << datasize/(end-start)/1.0e6 << \"\\n\";\n",
    "      }\n",
    "      delete [] data;\n",
    "    }\n",
    "    ```\n",
    "\n",
    "2.  \\[Minihomework\\] Mesuring the bandwidth and comparing with simple\n",
    "    send and recv\n",
    "\n",
    "    ``` cpp\n",
    "    #include \"mpi.h\"\n",
    "    #include <iostream>\n",
    "    #include <cstdlib>\n",
    "    #include <algorithm>\n",
    "    #include <numeric>\n",
    "\n",
    "    void send_data_collective(int size, int pid, int np);\n",
    "    void send_data_point_to_point(int size, int pid, int np);\n",
    "\n",
    "    int main(int argc, char **argv)\n",
    "    {\n",
    "      int np, pid;\n",
    "\n",
    "      /* MPI setup */\n",
    "      MPI_Init(&argc, &argv);\n",
    "      MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
    "      MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
    "\n",
    "      const int SIZE = std::atoi(argv[1]);\n",
    "      send_data_collective(SIZE, pid, np);\n",
    "\n",
    "      MPI_Finalize();\n",
    "\n",
    "      return 0;\n",
    "    }\n",
    "\n",
    "    void send_data_point_to_point(int size, int pid, int np)\n",
    "    {\n",
    "    // ???\n",
    "    }\n",
    "\n",
    "    void send_data_collective(int size, int pid, int np)\n",
    "    {\n",
    "      // create data buffer (all processes)\n",
    "      double * data = new double [size];\n",
    "      std::iota(data, data+size, 0.0);\n",
    "      // send data to all processes\n",
    "      int root = 0;\n",
    "      double start = MPI_Wtime();\n",
    "      MPI_Bcast(&data[0], size, MPI_DOUBLE, root, MPI_COMM_WORLD);\n",
    "      double end = MPI_Wtime();\n",
    "      // print size, time, bw in root\n",
    "      if (0 == pid) {\n",
    "        int datasize = sizeof(double)*size;\n",
    "        std::cout << datasize << \"\\t\" << (end-start) << \"\\t\"\n",
    "                  << datasize/(end-start)/1.0e6 << \"\\n\";\n",
    "      }\n",
    "      delete [] data;\n",
    "    }\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f7de8",
   "metadata": {},
   "source": [
    "\n",
    "### MPI<sub>Bcast</sub>, MPI<sub>Reduce</sub>\n",
    "\n",
    "In this case we will read some data in process 0 , then broadcast it to\n",
    "other processes, and finally reduce back some result (an integral , in\n",
    "this example).\n",
    "\n",
    "``` cpp\n",
    "#include \"mpi.h\"\n",
    "#include <cstdio>\n",
    "\n",
    "/* Problem parameters */\n",
    "double f(double x);\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  /* MPI Variables */\n",
    "  int dest, noProcesses, processId, src, tag;\n",
    "  MPI_Status status;\n",
    "\n",
    "  /* problem variables */\n",
    "  int i;\n",
    "  double area, at, heigth, width, total, range, lower;\n",
    "  int numberRects;\n",
    "  double lowerLimit;\n",
    "  double upperLimit;\n",
    "\n",
    "\n",
    "  /* MPI setup */\n",
    "  MPI_Init(&argc, &argv);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &noProcesses);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &processId);\n",
    "\n",
    "  /* read, send, and receive parameters */\n",
    "  tag = 0;\n",
    "  if (0 == processId) {\n",
    "    fprintf(stderr, \"Enter number of steps: \\n\");\n",
    "    scanf(\"%d\", &numberRects);\n",
    "    fprintf(stderr, \"Enter lower limit: \\n\");\n",
    "    scanf(\"%lf\", &lowerLimit);\n",
    "    fprintf(stderr, \"Enter upper limit: \\n\");\n",
    "    scanf(\"%lf\", &upperLimit);\n",
    "  }\n",
    "\n",
    "  /* Broadcast the read data parameters*/\n",
    "  MPI_Bcast(&numberRects, 1, MPI_INT,    0, MPI_COMM_WORLD);\n",
    "  MPI_Bcast(&lowerLimit,  1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
    "  MPI_Bcast(&upperLimit,  1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
    "\n",
    "  /* Adjust problem size for sub-process */\n",
    "  range = (upperLimit - lowerLimit) / noProcesses;\n",
    "  width = range / numberRects;\n",
    "  lower = lowerLimit + range*processId;\n",
    "\n",
    "  /* Calculate area for subproblem */\n",
    "  area = 0.0;\n",
    "  for (i = 0; i < numberRects; ++i) {\n",
    "    at = lower + i*width + width/2.0;\n",
    "    heigth = f(at);\n",
    "    area = area + width*heigth;\n",
    "  }\n",
    "\n",
    "  /* Collect and reduce data */\n",
    "  MPI_Reduce(&area, &total, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
    "\n",
    "  /* print results */\n",
    "  if (0 == processId) { /* Master */\n",
    "    std::fprintf(stderr, \"The area from %g to %g is : %25.16e\\n\", lowerLimit, upperLimit, total);\n",
    "  }\n",
    "\n",
    "  /* finish */\n",
    "  MPI_Finalize();\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "double f(double x) {\n",
    "  return x*x;\n",
    "}\n",
    "```\n",
    "\n",
    "The eight functions `MPI_Init`, `MPI_Comm_size`, `MPI_Comm_rank`,\n",
    "`MPI_Send`, `MPI_Recv`, `MPI_Bcast`, `MPI_Reduce`, and `MPI_Finalize`\n",
    "are the core of the MPI programming. You are now and MPI programmer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993d459",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercises\n",
    "\n",
    "1.  Modularize the previous integral code with functions. Basically use\n",
    "    the same code as with send/recv but adapt it to use bcast/reduce.\n",
    "2.  Do the same for the problem of computing the L2 norm (Euclidean\n",
    "    norm) of a very large vector. Fill it with ones and test the answer.\n",
    "    What kind of cpu time scaling do you get as function of the number\n",
    "    of processes, with N = 100000?\n",
    "3.  Filling randomly a matrix with probability $p$: Make process 0 send\n",
    "    the value of $p$ to all processes. Also, distribute the matrix (only\n",
    "    one slice per process). Fill the matrix at each process with prob\n",
    "    $p$. To verify, compute, at each process, the total places filled\n",
    "    and send it to process 0. Add up all of them and print (from\n",
    "    process 0) both the total proportion of filled sites (must be close\n",
    "    to $p$) and the full matrix. Think if the algorithm that you devised\n",
    "    to detect percolant clusters can be parallelized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff303f",
   "metadata": {},
   "source": [
    "\n",
    "## More examples : `MPI_Gather`, `MPI_Scatter`\n",
    "\n",
    "Check:\n",
    "\n",
    "-   <https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/>\n",
    "-   <http://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture29.pdf>\n",
    "-   <https://en.wikipedia.org/wiki/Collective_operation>\n",
    "\n",
    "Functions :\n",
    "\n",
    "-   `MPI_Scatter:` Sends parts of a given data to corresponding\n",
    "    processes. Example: Array 16 doubles. You have 4 processes. Then\n",
    "    this functions sends the first 4 doubles to rank 0, the second 4\n",
    "    doubles to rank 1, and so on. Note: The original array must exists\n",
    "    on the sending process, so this might limit its size.\n",
    "-   `MPI_Gather:` It is the opposite. Recovers data from all processes\n",
    "    and stores it on the root process.\n",
    "\n",
    "Now let's do some a program to practice them. This program will compute\n",
    "the average of a set of random numbers. We will try not to share the\n",
    "full array of random numbers, but instead to make each process create\n",
    "its own array. We will use a Gaussian random number generator with mean\n",
    "1.23 and sigma 0.45:\n",
    "\n",
    "-   Share with all processes the local array size (`N/np`, where `N` is\n",
    "    the array size and `np` is the number of processes), the mean and\n",
    "    the sigma.\n",
    "-   Share with all process the seed for the local random number\n",
    "    generator. The seed might be the rank. Store the seeds on a local\n",
    "    array and then send them from rank 0.\n",
    "-   Compute the local sum on each process.\n",
    "-   Get the local sum by storing it on the seeds array (for now, don't\n",
    "    reduce).\n",
    "-   Compute the mean and print it.\n",
    "\n",
    "There are many other collective communications functions worth\n",
    "mentioning (<https://en.wikipedia.org/wiki/Collective_operation> ,\n",
    "<https://www.mpi-forum.org/docs/mpi-1.1/mpi-11-html/node64.html>,\n",
    "<https://www.rookiehpc.com/mpi/docs/index.php> ):\n",
    "\n",
    "-   MPI<sub>Scatter</sub> and MPI<sub>Gather</sub>\n",
    "-   MPI<sub>AllGather</sub> and MPI<sub>Alltoall</sub>\n",
    "-   MPI<sub>Scatterv</sub>\n",
    "-   MPI<sub>Reduce</sub>, MPI<sub>Scan</sub>, MPI<sub>ExScan</sub>,\n",
    "    Reduce<sub>Scatter</sub>\n",
    "\n",
    "**Exercise:** Each group will implement a working example of each of the\n",
    "following functions, as follows:\n",
    "<https://www.rookiehpc.com/mpi/docs/index.php>\n",
    "\n",
    "| Group | Example                                                                     |\n",
    "|-------|-----------------------------------------------------------------------------|\n",
    "| 1     | MPI<sub>scatter</sub> and MPI<sub>Gather</sub>, MPI<sub>allgather</sub>     |\n",
    "| 2     | MPI<sub>Scatterv</sub>, MPI<sub>Gatherv</sub>                               |\n",
    "| 3     | MPI<sub>Alltoall</sub>, MPI<sub>alltoallv</sub>                             |\n",
    "| 4     | MPI<sub>Reduce</sub>, MPI<sub>Reducescatter</sub>                           |\n",
    "| 5     | MPI<sub>Scan</sub>, MPI<sub>Exscan</sub>                                    |\n",
    "| 6     | MPI<sub>All</sub>\\`Reduce with many MPI<sub>OP</sub> (min, max, sum , mult) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70678a50",
   "metadata": {},
   "source": [
    "\n",
    "### Collective comms exercises\n",
    "\n",
    "1.  Revisit all point-to-point comms exercises and implement them using\n",
    "    collective comms if possible.\n",
    "\n",
    "2.  Ring code REF: Gavin, EPCC\n",
    "\n",
    "    -   Use a Collective Communications routine to produce the same\n",
    "        result as your Ring Code so that only Task 0 ‘knows’ the answer.\n",
    "    -   Now change the Collective Communications call so all tasks\n",
    "        ‘know’ the answer.\n",
    "    -   Can you think of another way to implement the collective\n",
    "        communications routine than the Ring Code?\n",
    "    -   Compare the execution times for both the Ring Code and the\n",
    "        associated Collective Communication routine.\n",
    "    -   Which is faster?\n",
    "    -   Why do the times change so much when you run it again and again?\n",
    "    -   What happens when the size of the message goes from 1\n",
    "        MPI<sub>INT</sub> to 10000 MPI<sub>INTs</sub>?!\n",
    "\n",
    "3.  Scaling for mpi integral Compute the scaling of the mpi integral\n",
    "    code when using point to point and collective communications, in the\n",
    "    same machine. Then do the same for two machines, dividing the number\n",
    "    of processes among them.\n",
    "\n",
    "4.  You will compute the polynomial $f(x) = x + x^{2} + x^{3} +\n",
    "      x^{4} + \\ldots + x^{N}$. Write an mpi program that computes this\n",
    "    in $N$ processes and prints the final value on master (id = 0) for\n",
    "    $x = 0.987654$ and $N = 25$. Use broadcast communications when\n",
    "    possible.\n",
    "\n",
    "5.  Compute the mean of a large array by distributing it on several\n",
    "    processes. Your program must print the partial sums on each node and\n",
    "    the final mean on master.\n",
    "\n",
    "6.  Compute the value of $\\pi$ by using the montecarlo method: Imagine a\n",
    "    circle of radius 1. Its area would be $\\pi$. Imagine now the circle\n",
    "    inscribed inside a square of length 2. Now compute the area ($\\pi$)\n",
    "    of the circle by using a simple Monte-Carlo procedure: just throw\n",
    "    some random points inside the square ($N_s$) and count the number of\n",
    "    points that fall inside the circle ($N_c$). The ratio of the areas\n",
    "    is related with the ratio of inner points as $A_c/A_s\n",
    "      \\simeq N_c/N_s$, therefore $A_C \\simeq A_s N_c/N_s$. Use as many\n",
    "    processes as possible to increase the precision. At the end print\n",
    "    the total number of points tested, the value of $\\pi$ found, and the\n",
    "    percent difference with the expected value, $\\Delta$. Plot $\\Delta$\n",
    "    as function of $N_p$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd0521",
   "metadata": {},
   "source": [
    "\n",
    "## Practical example: Parallel Relaxation method\n",
    "\n",
    "In this example we will parallelize the relaxation method by using MPI.\n",
    "We will decompose the domain, add some ghost zones and finally transmit\n",
    "some information between different processors. Our tasks will be as\n",
    "follows:\n",
    "\n",
    "1.  Probar la version serial.\n",
    "2.  Por cada proceso, crear el arreglo local incluyendo los ghosts:\n",
    "    Probar llenando esta matriz con el pid y luego imprimir la matriz\n",
    "    completa incluyendo y sin incluir los ghost (inicialización)\n",
    "3.  Paralelizar la inicializacion e imprimir comparando con la version\n",
    "    serial.\n",
    "4.  Paralelizar las condiciones de frontera\n",
    "5.  Comunicacion entre procesos: Enviar y recibir de vecinos y guardar\n",
    "    en las zonas ghost. Imprimir para verificar. Introducir y probar\n",
    "    diversas formas de comunicación, como Send y Recv, llamadas no\n",
    "    bloqueantes, Sendrecv, y MPI<sub>Neighborªlltoallw</sub> con\n",
    "    MPI<sub>Createcart</sub>.\n",
    "6.  Paralelizar la evolucion. Tener en cuenta las condiciones de\n",
    "    frontera.\n",
    "7.  Medir metricas paralelas y medir el tiempo de comunicacion.\n",
    "8.  Extender a codiciones de frontera internas Usar un arreglo booleano\n",
    "    que indique si una celda es frintera\n",
    "9.  Calcular tambien el campo y/o densidades de carga.\n",
    "\n",
    "To decompose the domain, we will split the original matriz in horizontal\n",
    "slices, one per processor (we assume $N%n_p == 0$). The data array, for\n",
    "each processor, we will add two ghost rows to copy the boundary\n",
    "information in order to make the evolution locally. Therefore, after\n",
    "each update iteration, sync to the ghosts zones is needed. Locally each\n",
    "array will be of size $(N_{l} +\n",
    "2)\\times N$, where $N_l = N/n_p$ and there are 2 ghost rows. The\n",
    "following diagram illustrates the process:\n",
    "<img src=\"fig/Laplace-domain-ghostscells.png\" class=centerimg50 >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf46ba4",
   "metadata": {},
   "source": [
    "\n",
    "### Serial version\n",
    "\n",
    "This version solves the Laplace equation by using the Jacbi-Gauss-Seidel\n",
    "relaxation method and prints gnuplot commands to create an animation.\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <cmath>\n",
    "\n",
    "// constants\n",
    "const double DELTA = 0.05;\n",
    "const double L = 1.479;\n",
    "const int N = int(L/DELTA)+1;\n",
    "const int STEPS = 200;\n",
    "\n",
    "typedef std::vector<double> Matrix; // alias\n",
    "\n",
    "void initial_conditions(Matrix & m);\n",
    "void boundary_conditions(Matrix & m);\n",
    "void evolve(Matrix & m);\n",
    "void print(const Matrix & m);\n",
    "void init_gnuplot(void);\n",
    "void plot_gnuplot(const Matrix & m);\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  Matrix data(N*N);\n",
    "  initial_conditions(data);\n",
    "  boundary_conditions(data);\n",
    "\n",
    "  //init_gnuplot();\n",
    "  for (int istep = 0; istep < STEPS; ++istep) {\n",
    "    evolve(data);\n",
    "    //plot_gnuplot(data);\n",
    "    if (istep == STEPS-1) {\n",
    "      print(data);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "void initial_conditions(Matrix & m)\n",
    "{\n",
    "  for(int ii=0; ii<N; ++ii) {\n",
    "    for(int jj=0; jj<N; ++jj) {\n",
    "      m[ii*N + jj] = 1.0;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "void boundary_conditions(Matrix & m)\n",
    "{\n",
    "  int ii = 0, jj = 0;\n",
    "\n",
    "  ii = 0;\n",
    "  for (jj = 0; jj < N; ++jj)\n",
    "    m[ii*N + jj] = 100;\n",
    "\n",
    "  ii = N-1;\n",
    "  for (jj = 0; jj < N; ++jj)\n",
    "    m[ii*N + jj] = 0;\n",
    "\n",
    "  jj = 0;\n",
    "  for (ii = 1; ii < N-1; ++ii)\n",
    "    m[ii*N + jj] = 0;\n",
    "\n",
    "  jj = N-1;\n",
    "  for (ii = 1; ii < N-1; ++ii)\n",
    "    m[ii*N + jj] = 0;\n",
    "}\n",
    "\n",
    "void evolve(Matrix & m)\n",
    "{\n",
    "  for(int ii=0; ii<N; ++ii) {\n",
    "    for(int jj=0; jj<N; ++jj) {\n",
    "      // check if boundary\n",
    "      if(ii == 0) continue;\n",
    "      if(ii == N-1) continue;\n",
    "      if(jj == 0) continue;\n",
    "      if(jj == N-1) continue;\n",
    "      // evolve non boundary\n",
    "      m[ii*N+jj] = (m[(ii+1)*N + jj] +\n",
    "                    m[(ii-1)*N + jj] +\n",
    "                    m[ii*N + jj + 1] +\n",
    "                    m[ii*N + jj - 1] )/4.0;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void print(const Matrix & m)\n",
    "{\n",
    "  for(int ii=0; ii<N; ++ii) {\n",
    "    for(int jj=0; jj<N; ++jj) {\n",
    "      std::cout << ii*DELTA << \" \" << jj*DELTA << \" \" <<  m[ii*N + jj] << \"\\n\";\n",
    "    }\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "}\n",
    "\n",
    "void init_gnuplot(void)\n",
    "{\n",
    "  std::cout << \"set contour \" << std::endl;\n",
    "  std::cout << \"set terminal gif animate \" << std::endl;\n",
    "  std::cout << \"set out 'anim.gif' \" << std::endl;\n",
    "}\n",
    "\n",
    "void plot_gnuplot(const Matrix & m)\n",
    "{\n",
    "  std::cout << \"splot '-' w pm3d \" << std::endl;\n",
    "  print(m);\n",
    "  std::cout << \"e\" << std::endl;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "**Exercise:** Now just try to add the very basic mpi calls without\n",
    "actually parallelizing:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f49ef",
   "metadata": {},
   "source": [
    "\n",
    "### Parallelize both initial and boundary conditions\n",
    "\n",
    "The folowing code shows a possible parallelization for the initial\n",
    "conditions, plus some auxiliary functions to print the matrix using\n",
    "point to poin comms:\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <cmath>\n",
    "#include <iomanip>\n",
    "#include \"mpi.h\"\n",
    "\n",
    "// constants\n",
    "// const double DELTA = 0.05;\n",
    "// const double L = 1.479;\n",
    "// const int N = int(L/DELTA)+1;\n",
    "// const int STEPS = 200;\n",
    "\n",
    "typedef std::vector<double> Matrix; // alias\n",
    "\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols);\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols);\n",
    "void evolve(Matrix & m, int nrows, int ncols);\n",
    "void print(const Matrix & m, double delta, int nrows, int ncols);\n",
    "void init_gnuplot(void);\n",
    "void plot_gnuplot(const Matrix & m, double delta, int nrows, int ncols);\n",
    "\n",
    "// parallel versions\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void print_screen(const Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void print_slab(const Matrix & m, int nrows, int ncols);\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  MPI_Init(&argc, &argv);\n",
    "  int pid, np;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
    "\n",
    "  const int N = std::atoi(argv[1]);\n",
    "  const double L = std::atof(argv[2]);\n",
    "  const int STEPS = std::atoi(argv[3]);\n",
    "  const double DELTA = L/N;\n",
    "\n",
    "  // problem partition\n",
    "  int NCOLS = N, NROWS = N/np + 2; // include ghosts\n",
    "  Matrix data(NROWS*NCOLS); // include ghosts cells\n",
    "  initial_conditions(data, NROWS, NCOLS, pid, np);\n",
    "  if (0 == pid) {std::cout << \" After initial conditions ...\\n\";}\n",
    "  print_screen(data, NROWS, NCOLS, pid, np); // todo\n",
    "  boundary_conditions(data, NROWS, NCOLS, pid, np); // todo\n",
    "  if (0 == pid) {std::cout << \" After boundary conditions ...\\n\";}\n",
    "  print_screen(data, NROWS, NCOLS, pid, np); // todo\n",
    "\n",
    "  /*\n",
    "  // Serial version\n",
    "  Matrix data(N*N);\n",
    "  initial_conditions(data, N, N, ...);\n",
    "  print_screen(...);\n",
    "  boundary_conditions(data, N, N, ...);\n",
    "  init_gnuplot();\n",
    "  for (int istep = 0; istep < STEPS; ++istep) {\n",
    "  evolve(data, N, N);\n",
    "  plot_gnuplot(data, DELTA, N, N);\n",
    "  }\n",
    "  */\n",
    "\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "/////////////////////////////////////////////////////\n",
    "// Parallel implementations\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // same task for all pids, but fill with the pids to distinguish among thems\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      m[ii*ncols + jj] = pid;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // TODO\n",
    "  // Take into account each pid\n",
    "\n",
    "}\n",
    "\n",
    "void print_screen(const Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // TODO\n",
    "\n",
    "}\n",
    "\n",
    "void print_slab(const Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  // ignore ghosts\n",
    "  for(int ii = 1; ii < nrows-1; ++ii) {\n",
    "    for(int jj = 0; jj < ncols; ++jj) {\n",
    "      std::cout << std::setw(3) <<  m[ii*ncols + jj] << \" \";\n",
    "    }\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "}\n",
    "\n",
    "/////////////////////////////////////////////////////\n",
    "// SERIAL VERSIONS\n",
    "\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      m[ii*ncols + jj] = 1.0;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  int ii = 0, jj = 0;\n",
    "\n",
    "  ii = 0;\n",
    "  for (jj = 0; jj < ncols; ++jj)\n",
    "    m[ii*ncols + jj] = 100;\n",
    "\n",
    "  ii = nrows-1;\n",
    "  for (jj = 0; jj < ncols; ++jj)\n",
    "    m[ii*ncols + jj] = 0;\n",
    "\n",
    "  jj = 0;\n",
    "  for (ii = 1; ii < nrows-1; ++ii)\n",
    "    m[ii*ncols + jj] = 0;\n",
    "\n",
    "  jj = ncols-1;\n",
    "  for (ii = 1; ii < nrows-1; ++ii)\n",
    "    m[ii*ncols + jj] = 0;\n",
    "}\n",
    "\n",
    "void evolve(Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      // check if boundary\n",
    "      if(ii == 0) continue;\n",
    "      if(ii == nrows-1) continue;\n",
    "      if(jj == 0) continue;\n",
    "      if(jj == ncols-1) continue;\n",
    "      // evolve non boundary\n",
    "      m[ii*ncols+jj] = (m[(ii+1)*ncols + jj] +\n",
    "                        m[(ii-1)*ncols + jj] +\n",
    "                        m[ii*ncols + jj + 1] +\n",
    "                        m[ii*ncols + jj - 1] )/4.0;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void print(const Matrix & m, double delta, int nrows, int ncols)\n",
    "{\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      std::cout << ii*delta << \" \" << jj*delta << \" \" <<  m[ii*ncols + jj] << \"\\n\";\n",
    "    }\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "}\n",
    "\n",
    "void init_gnuplot(void)\n",
    "{\n",
    "  std::cout << \"set contour \" << std::endl;\n",
    "  //std::cout << \"set terminal gif animate \" << std::endl;\n",
    "  //std::cout << \"set out 'anim.gif' \" << std::endl;\n",
    "}\n",
    "\n",
    "void plot_gnuplot(const Matrix & m, double delta, int nrows, int ncols)\n",
    "{\n",
    "  std::cout << \"splot '-' w pm3d \" << std::endl;\n",
    "  print(m, delta, nrows, ncols);\n",
    "  std::cout << \"e\" << std::endl;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Complete the needed functions to paralelize the boundary conditions and\n",
    "printing.\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <cmath>\n",
    "#include <iomanip>\n",
    "#include \"mpi.h\"\n",
    "\n",
    "typedef std::vector<double> Matrix; // alias\n",
    "\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols);\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols);\n",
    "void evolve(Matrix & m, int nrows, int ncols);\n",
    "void print(const Matrix & m, double delta, int nrows, int ncols);\n",
    "void init_gnuplot(void);\n",
    "void plot_gnuplot(const Matrix & m, double delta, int nrows, int ncols);\n",
    "\n",
    "// parallel versions\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void print_screen(const Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void print_slab(const Matrix & m, int nrows, int ncols);\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  MPI_Init(&argc, &argv);\n",
    "  int pid, np;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
    "\n",
    "  const int N = std::atoi(argv[1]);\n",
    "  const double L = std::atof(argv[2]);\n",
    "  const int STEPS = std::atoi(argv[3]);\n",
    "  const double DELTA = L/N;\n",
    "\n",
    "  // problem partition\n",
    "  int NCOLS = N, NROWS = N/np + 2; // include ghosts\n",
    "  Matrix data(NROWS*NCOLS); // include ghosts cells\n",
    "  initial_conditions(data, NROWS, NCOLS, pid, np);\n",
    "  if (0 == pid) {std::cout << \" After initial conditions ...\\n\";}\n",
    "  print_screen(data, NROWS, NCOLS, pid, np); // todo\n",
    "  boundary_conditions(data, NROWS, NCOLS, pid, np); // todo\n",
    "  if (0 == pid) {std::cout << \" After boundary conditions ...\\n\";}\n",
    "  print_screen(data, NROWS, NCOLS, pid, np); // todo\n",
    "\n",
    "  /*\n",
    "  // Serial version\n",
    "  Matrix data(N*N);\n",
    "  initial_conditions(data, N, N, ...);\n",
    "  print_screen(...);\n",
    "  boundary_conditions(data, N, N, ...);\n",
    "  init_gnuplot();\n",
    "  for (int istep = 0; istep < STEPS; ++istep) {\n",
    "  evolve(data, N, N);\n",
    "  plot_gnuplot(data, DELTA, N, N);\n",
    "  }\n",
    "  */\n",
    "\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "/////////////////////////////////////////////////////\n",
    "// Parallel implementations\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // same task for all pids, but fill with the pids to distinguish among thems\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      m[ii*ncols + jj] = pid;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // Take into account each pid\n",
    "  <<boundary-student>>\n",
    "}\n",
    "\n",
    "void print_screen(const Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  <<print-student>>\n",
    "\n",
    "}\n",
    "\n",
    "void print_slab(const Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  // ignore ghosts\n",
    "  for(int ii = 1; ii < nrows-1; ++ii) {\n",
    "    for(int jj = 0; jj < ncols; ++jj) {\n",
    "      std::cout << std::setw(3) <<  m[ii*ncols + jj] << \" \";\n",
    "    }\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "}\n",
    "\n",
    "/////////////////////////////////////////////////////\n",
    "// SERIAL VERSIONS\n",
    "\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      m[ii*ncols + jj] = 1.0;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  int ii = 0, jj = 0;\n",
    "\n",
    "  ii = 0;\n",
    "  for (jj = 0; jj < ncols; ++jj)\n",
    "    m[ii*ncols + jj] = 100;\n",
    "\n",
    "  ii = nrows-1;\n",
    "  for (jj = 0; jj < ncols; ++jj)\n",
    "    m[ii*ncols + jj] = 0;\n",
    "\n",
    "  jj = 0;\n",
    "  for (ii = 1; ii < nrows-1; ++ii)\n",
    "    m[ii*ncols + jj] = 0;\n",
    "\n",
    "  jj = ncols-1;\n",
    "  for (ii = 1; ii < nrows-1; ++ii)\n",
    "    m[ii*ncols + jj] = 0;\n",
    "}\n",
    "\n",
    "void evolve(Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      // check if boundary\n",
    "      if(ii == 0) continue;\n",
    "      if(ii == nrows-1) continue;\n",
    "      if(jj == 0) continue;\n",
    "      if(jj == ncols-1) continue;\n",
    "      // evolve non boundary\n",
    "      m[ii*ncols+jj] = (m[(ii+1)*ncols + jj] +\n",
    "                        m[(ii-1)*ncols + jj] +\n",
    "                        m[ii*ncols + jj + 1] +\n",
    "                        m[ii*ncols + jj - 1] )/4.0;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void print(const Matrix & m, double delta, int nrows, int ncols)\n",
    "{\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      std::cout << ii*delta << \" \" << jj*delta << \" \" <<  m[ii*ncols + jj] << \"\\n\";\n",
    "    }\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "}\n",
    "\n",
    "void init_gnuplot(void)\n",
    "{\n",
    "  std::cout << \"set contour \" << std::endl;\n",
    "  //std::cout << \"set terminal gif animate \" << std::endl;\n",
    "  //std::cout << \"set out 'anim.gif' \" << std::endl;\n",
    "}\n",
    "\n",
    "void plot_gnuplot(const Matrix & m, double delta, int nrows, int ncols)\n",
    "{\n",
    "  std::cout << \"splot '-' w pm3d \" << std::endl;\n",
    "  print(m, delta, nrows, ncols);\n",
    "  std::cout << \"e\" << std::endl;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "You should get something like:\n",
    "\n",
    "``` shell\n",
    "cd codes\n",
    "mpic++ MPI_laplace_v1.cpp\n",
    "mpirun -np 4 --oversubscribe ./a.out ./a.out 12 1.4705 1\n",
    "```\n",
    "\n",
    "``` shell\n",
    "After initial conditions ...\n",
    " 0   0   0   0   0   0   0   0   0   0   0   0\n",
    " 0   0   0   0   0   0   0   0   0   0   0   0\n",
    " 0   0   0   0   0   0   0   0   0   0   0   0\n",
    " 1   1   1   1   1   1   1   1   1   1   1   1\n",
    " 1   1   1   1   1   1   1   1   1   1   1   1\n",
    " 1   1   1   1   1   1   1   1   1   1   1   1\n",
    " 2   2   2   2   2   2   2   2   2   2   2   2\n",
    " 2   2   2   2   2   2   2   2   2   2   2   2\n",
    " 2   2   2   2   2   2   2   2   2   2   2   2\n",
    " 3   3   3   3   3   3   3   3   3   3   3   3\n",
    " 3   3   3   3   3   3   3   3   3   3   3   3\n",
    " 3   3   3   3   3   3   3   3   3   3   3   3\n",
    "After boundary conditions ...\n",
    " 0 100 100 100 100 100 100 100 100 100 100   0\n",
    " 0   0   0   0   0   0   0   0   0   0   0   0\n",
    " 0   0   0   0   0   0   0   0   0   0   0   0\n",
    " 0   1   1   1   1   1   1   1   1   1   1   0\n",
    " 0   1   1   1   1   1   1   1   1   1   1   0\n",
    " 0   1   1   1   1   1   1   1   1   1   1   0\n",
    " 0   2   2   2   2   2   2   2   2   2   2   0\n",
    " 0   2   2   2   2   2   2   2   2   2   2   0\n",
    " 0   2   2   2   2   2   2   2   2   2   2   0\n",
    " 0   3   3   3   3   3   3   3   3   3   3   0\n",
    " 0   3   3   3   3   3   3   3   3   3   3   0\n",
    " 0   0   0   0   0   0   0   0   0   0   0   0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c32662",
   "metadata": {},
   "source": [
    "\n",
    "### Communication\n",
    "\n",
    "Here we explore how to implement the communication among the processes\n",
    "using the ring topology. Your task is to implement the communication in\n",
    "the following ways using\n",
    "\n",
    "-   Typical Send and Recv calls\n",
    "-   Using non-blocking sends\n",
    "-   Using the function MPI<sub>sendrecv</sub>\n",
    "-   Using MPI<sub>createcart</sub> and MPI<sub>neighbor</sub> alltoallw\n",
    "\n",
    "Independet of the way it is implement, on step of comms must look like\n",
    "(ghost cells are printed and a new line added per process)\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Implement the communication pattern using simple senc and recv\n",
    "\n",
    "```\n",
    "TODO\n",
    "```\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <cmath>\n",
    "#include <iomanip>\n",
    "#include \"mpi.h\"\n",
    "\n",
    "typedef std::vector<double> Matrix; // alias\n",
    "\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols);\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols);\n",
    "void evolve(Matrix & m, int nrows, int ncols);\n",
    "void print(const Matrix & m, double delta, int nrows, int ncols);\n",
    "void init_gnuplot(void);\n",
    "void plot_gnuplot(const Matrix & m, double delta, int nrows, int ncols);\n",
    "\n",
    "// parallel versions\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void print_screen(const Matrix & m, int nrows, int ncols, int pid, int np, bool ghosts = false);\n",
    "void print_slab(const Matrix & m, int nrows, int ncols, bool ghosts);\n",
    "void send_rows(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void send_rows_non_blocking(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void send_rows_sendrecv(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void send_rows_topology(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  MPI_Init(&argc, &argv);\n",
    "  int pid, np;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
    "\n",
    "  const int N = std::atoi(argv[1]);\n",
    "  const double L = std::atof(argv[2]);\n",
    "  const int STEPS = std::atoi(argv[3]);\n",
    "  const double DELTA = L/N;\n",
    "\n",
    "  // problem partition\n",
    "  int NCOLS = N, NROWS = N/np + 2; // include ghosts\n",
    "  Matrix data(NROWS*NCOLS); // include ghosts cells\n",
    "  initial_conditions(data, NROWS, NCOLS, pid, np);\n",
    "  if (0 == pid) {std::cout << \" After initial conditions ...\\n\";}\n",
    "  print_screen(data, NROWS, NCOLS, pid, np); // todo\n",
    "\n",
    "  boundary_conditions(data, NROWS, NCOLS, pid, np); // todo\n",
    "  if (0 == pid) {std::cout << \" After boundary conditions ...\\n\";}\n",
    "  print_screen(data, NROWS, NCOLS, pid, np, true); // todo\n",
    "\n",
    "  send_rows(data, NROWS, NCOLS, pid, np); // todo\n",
    "  //send_rows_non_blocking(data, NROWS, NCOLS, pid, np); // todo\n",
    "  //send_rows_sendrecv(data, NROWS, NCOLS, pid, np); // todo\n",
    "  //send_rows_topology(data, NROWS, NCOLS, pid, np); // todo\n",
    "  if (0 == pid) {std::cout << \" After one comm ...\\n\";}\n",
    "  print_screen(data, NROWS, NCOLS, pid, np, true); // todo\n",
    "\n",
    "  /*\n",
    "  // Serial version\n",
    "  Matrix data(N*N);\n",
    "  initial_conditions(data, N, N, ...);\n",
    "  print_screen(...);\n",
    "  boundary_conditions(data, N, N, ...);\n",
    "  init_gnuplot();\n",
    "  for (int istep = 0; istep < STEPS; ++istep) {\n",
    "  evolve(data, N, N);\n",
    "  plot_gnuplot(data, DELTA, N, N);\n",
    "  }\n",
    "  */\n",
    "\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "/////////////////////////////////////////////////////\n",
    "// Parallel implementations\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // same task for all pids, but fill with the pids to distinguish among thems\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      m[ii*ncols + jj] = pid;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  <<boundary-student>>\n",
    "}\n",
    "\n",
    "void print_screen(const Matrix & m, int nrows, int ncols, int pid, int np, bool ghosts)\n",
    "{\n",
    "  <<print-student>>\n",
    "}\n",
    "\n",
    "void print_slab(const Matrix & m, int nrows, int ncols, bool ghosts)\n",
    "{\n",
    "  int imin = 0, imax = nrows;\n",
    "  if (false == ghosts) {\n",
    "    imin = 1; imax = nrows-1;\n",
    "  }\n",
    "  for(int ii = imin; ii < imax; ++ii) {\n",
    "    for(int jj = 0; jj < ncols; ++jj) {\n",
    "      std::cout << std::setw(3) <<  m[ii*ncols + jj] << \" \";\n",
    "    }\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "  std::cout << \"\\n\";\n",
    "}\n",
    "\n",
    "void send_rows(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  <<send-rows-student>>\n",
    "}\n",
    "\n",
    "void send_rows_non_blocking(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // send data forwards\n",
    "  MPI_Request r1;\n",
    "  if (pid <= np-2) {\n",
    "    //MPI_Send(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD);\n",
    "    MPI_Isend(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD, &r1);\n",
    "  }\n",
    "  if (pid >= 1) {\n",
    "    MPI_Recv(&m[0], ncols, MPI_DOUBLE, pid-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "  // send data backwards\n",
    "  MPI_Request r2;\n",
    "  if (pid >= 1) {\n",
    "    MPI_Isend(&m[(1)*ncols], ncols, MPI_DOUBLE, pid-1, 1, MPI_COMM_WORLD, &r2);\n",
    "  }\n",
    "  if (pid <= np-2) {\n",
    "    MPI_Recv(&m[(nrows-1)*ncols], ncols, MPI_DOUBLE, pid+1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "\n",
    "  if (pid <= np-2) {\n",
    "    MPI_Wait(&r1, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "\n",
    "  if (pid >= 1) {\n",
    "    MPI_Wait(&r2, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "}\n",
    "\n",
    "void send_rows_sendrecv(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  if (0 == pid) {\n",
    "    MPI_Sendrecv(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0,\n",
    "                 &m[(nrows-1)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "  if (1 <= pid && pid <= np-2) {\n",
    "    // send data forwards\n",
    "    MPI_Sendrecv(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0,\n",
    "                 &m[(0)*ncols], ncols, MPI_DOUBLE, pid-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    // send data backwards\n",
    "    MPI_Sendrecv(&m[(1)*ncols], ncols, MPI_DOUBLE, pid-1, 0,\n",
    "                 &m[(nrows-1)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);  }\n",
    "\n",
    "  if (np-1 == pid) {\n",
    "    MPI_Sendrecv(&m[(1)*ncols], ncols, MPI_DOUBLE, pid-1, 0,\n",
    "                 &m[(0)*ncols], ncols, MPI_DOUBLE, pid-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "}\n",
    "\n",
    "void send_rows_topology(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // create a cartesian 1D topology representing the non-periodic ring\n",
    "  // https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture28.pdf\n",
    "  // https://www.codingame.com/playgrounds/47058/have-fun-with-mpi-in-c/mpi-process-topologies\n",
    "  // MPI_Cart_create(MPI_Comm old_comm, int ndims, const int *dims, const int *periods, int reorder, MPI_Comm *comm_cart)\n",
    "  int ndims = 1, reorder = 1;\n",
    "  int dimsize[ndims] {np};\n",
    "  int periods[ndims] {0};\n",
    "  MPI_Comm comm;\n",
    "  MPI_Cart_create(MPI_COMM_WORLD, 1, &dimsize[0], &periods[0], reorder, &comm);\n",
    "\n",
    "  // Now use the network topology to communicate data in one pass for each direction\n",
    "  // https://www.open-mpi.org/doc/v3.0/man3/MPI_Neighbor_alltoallw.3.php\n",
    "  // https://www.events.prace-ri.eu/event/967/contributions/1110/attachments/1287/2213/MPI_virtual_topologies.pdf\n",
    "  MPI_Datatype types[2] = {MPI_DOUBLE, MPI_DOUBLE};\n",
    "  // NOTE: scounts[0] goes with rcounts[1] (left and right)\n",
    "  int scounts[2] = {ncols, ncols};\n",
    "  int rcounts[2] = {ncols, ncols};\n",
    "  MPI_Aint sdispl[2]{0}, rdispl[2] {0};\n",
    "  MPI_Get_address(&m[0]+(nrows-2)*ncols, &sdispl[1]); // send to next\n",
    "  MPI_Get_address(&m[0]+(0)*ncols, &rdispl[0]); // receive from previous\n",
    "  MPI_Get_address(&m[0]+(1)*ncols, &sdispl[0]); // send to previous\n",
    "  MPI_Get_address(&m[0]+(nrows-1)*ncols, &rdispl[1]); // receive from next\n",
    "  MPI_Neighbor_alltoallw(MPI_BOTTOM, scounts, sdispl, types,\n",
    "                         MPI_BOTTOM, rcounts, rdispl, types, comm);\n",
    "}\n",
    "\n",
    "/////////////////////////////////////////////////////\n",
    "// SERIAL VERSIONS\n",
    "\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      m[ii*ncols + jj] = 1.0;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  int ii = 0, jj = 0;\n",
    "\n",
    "  ii = 0;\n",
    "  for (jj = 0; jj < ncols; ++jj)\n",
    "    m[ii*ncols + jj] = 100;\n",
    "\n",
    "  ii = nrows-1;\n",
    "  for (jj = 0; jj < ncols; ++jj)\n",
    "    m[ii*ncols + jj] = 0;\n",
    "\n",
    "  jj = 0;\n",
    "  for (ii = 1; ii < nrows-1; ++ii)\n",
    "    m[ii*ncols + jj] = 0;\n",
    "\n",
    "  jj = ncols-1;\n",
    "  for (ii = 1; ii < nrows-1; ++ii)\n",
    "    m[ii*ncols + jj] = 0;\n",
    "}\n",
    "\n",
    "void evolve(Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      // check if boundary\n",
    "      if(ii == 0) continue;\n",
    "      if(ii == nrows-1) continue;\n",
    "      if(jj == 0) continue;\n",
    "      if(jj == ncols-1) continue;\n",
    "      // evolve non boundary\n",
    "      m[ii*ncols+jj] = (m[(ii+1)*ncols + jj] +\n",
    "                        m[(ii-1)*ncols + jj] +\n",
    "                        m[ii*ncols + jj + 1] +\n",
    "                        m[ii*ncols + jj - 1] )/4.0;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void print(const Matrix & m, double delta, int nrows, int ncols)\n",
    "{\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      std::cout << ii*delta << \" \" << jj*delta << \" \" <<  m[ii*ncols + jj] << \"\\n\";\n",
    "    }\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "}\n",
    "\n",
    "void init_gnuplot(void)\n",
    "{\n",
    "  std::cout << \"set contour \" << std::endl;\n",
    "  //std::cout << \"set terminal gif animate \" << std::endl;\n",
    "  //std::cout << \"set out 'anim.gif' \" << std::endl;\n",
    "}\n",
    "\n",
    "void plot_gnuplot(const Matrix & m, double delta, int nrows, int ncols)\n",
    "{\n",
    "  std::cout << \"splot '-' w pm3d \" << std::endl;\n",
    "  print(m, delta, nrows, ncols);\n",
    "  std::cout << \"e\" << std::endl;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "// send data forwards\n",
    "if (pid <= np-2) {\n",
    "  MPI_Send(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD);\n",
    "}\n",
    "if (pid >= 1) {\n",
    "  MPI_Recv(&m[0], ncols, MPI_DOUBLE, pid-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "}\n",
    "// send data backwards\n",
    "if (pid >= 1) {\n",
    "  MPI_Send(&m[(1)*ncols], ncols, MPI_DOUBLE, pid-1, 1, MPI_COMM_WORLD);\n",
    "}\n",
    "if (pid <= np-2) {\n",
    "  MPI_Recv(&m[(nrows-1)*ncols], ncols, MPI_DOUBLE, pid+1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "}\n",
    "```\n",
    "\n",
    "``` shell\n",
    "❯ mpirun -np 6 --oversubscribe ./a.out 12 1.4705 1\n",
    " After initial conditions ...\n",
    "  0   0   0   0   0   0   0   0   0   0   0   0\n",
    "  0   0   0   0   0   0   0   0   0   0   0   0\n",
    "\n",
    "  1   1   1   1   1   1   1   1   1   1   1   1\n",
    "  1   1   1   1   1   1   1   1   1   1   1   1\n",
    "\n",
    "  2   2   2   2   2   2   2   2   2   2   2   2\n",
    "  2   2   2   2   2   2   2   2   2   2   2   2\n",
    "\n",
    "  3   3   3   3   3   3   3   3   3   3   3   3\n",
    "  3   3   3   3   3   3   3   3   3   3   3   3\n",
    "\n",
    "  4   4   4   4   4   4   4   4   4   4   4   4\n",
    "  4   4   4   4   4   4   4   4   4   4   4   4\n",
    "\n",
    "  5   5   5   5   5   5   5   5   5   5   5   5\n",
    "  5   5   5   5   5   5   5   5   5   5   5   5\n",
    "\n",
    " After boundary conditions ...\n",
    "  0 100 100 100 100 100 100 100 100 100 100   0\n",
    "  0   0   0   0   0   0   0   0   0   0   0   0\n",
    "\n",
    "  0   1   1   1   1   1   1   1   1   1   1   0\n",
    "  0   1   1   1   1   1   1   1   1   1   1   0\n",
    "\n",
    "  0   2   2   2   2   2   2   2   2   2   2   0\n",
    "  0   2   2   2   2   2   2   2   2   2   2   0\n",
    "\n",
    "  0   3   3   3   3   3   3   3   3   3   3   0\n",
    "  0   3   3   3   3   3   3   3   3   3   3   0\n",
    "\n",
    "  0   4   4   4   4   4   4   4   4   4   4   0\n",
    "  0   4   4   4   4   4   4   4   4   4   4   0\n",
    "\n",
    "  0   5   5   5   5   5   5   5   5   5   5   0\n",
    "  0   0   0   0   0   0   0   0   0   0   0   0\n",
    "\n",
    " After one comm ...\n",
    "100 100 100 100 100 100 100 100 100 100 100 100\n",
    "  0 100 100 100 100 100 100 100 100 100 100   0\n",
    "  0   0   0   0   0   0   0   0   0   0   0   0\n",
    "  0   1   1   1   1   1   1   1   1   1   1   0\n",
    "\n",
    "  0   0   0   0   0   0   0   0   0   0   0   0\n",
    "  0   1   1   1   1   1   1   1   1   1   1   0\n",
    "  0   1   1   1   1   1   1   1   1   1   1   0\n",
    "  0   2   2   2   2   2   2   2   2   2   2   0\n",
    "\n",
    "  0   1   1   1   1   1   1   1   1   1   1   0\n",
    "  0   2   2   2   2   2   2   2   2   2   2   0\n",
    "  0   2   2   2   2   2   2   2   2   2   2   0\n",
    "  0   3   3   3   3   3   3   3   3   3   3   0\n",
    "\n",
    "  0   2   2   2   2   2   2   2   2   2   2   0\n",
    "  0   3   3   3   3   3   3   3   3   3   3   0\n",
    "  0   3   3   3   3   3   3   3   3   3   3   0\n",
    "  0   4   4   4   4   4   4   4   4   4   4   0\n",
    "\n",
    "  0   3   3   3   3   3   3   3   3   3   3   0\n",
    "  0   4   4   4   4   4   4   4   4   4   4   0\n",
    "  0   4   4   4   4   4   4   4   4   4   4   0\n",
    "  0   5   5   5   5   5   5   5   5   5   5   0\n",
    "\n",
    "  0   4   4   4   4   4   4   4   4   4   4   0\n",
    "  0   5   5   5   5   5   5   5   5   5   5   0\n",
    "  0   0   0   0   0   0   0   0   0   0   0   0\n",
    "  0   0   0   0   0   0   0   0   0   0   0   0\n",
    "\n",
    "```\n",
    "\n",
    "**Exercise** For a very large array, compute the bandwidth of each\n",
    "procedure as a function of the number of process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9d870",
   "metadata": {},
   "source": [
    "\n",
    "### Evolution\n",
    "\n",
    "In this case we will just move form the serial simple evolution to\n",
    "comms+evolution at each time step\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <cmath>\n",
    "#include <iomanip>\n",
    "#include \"mpi.h\"\n",
    "\n",
    "typedef std::vector<double> Matrix; // alias\n",
    "\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols);\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols);\n",
    "void evolve(Matrix & m, int nrows, int ncols);\n",
    "void print(const Matrix & m, double delta, int nrows, int ncols);\n",
    "void init_gnuplot(void);\n",
    "void plot_gnuplot(const Matrix & m, double delta, int nrows, int ncols);\n",
    "\n",
    "// parallel versions\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void print_screen(const Matrix & m, int nrows, int ncols, int pid, int np, bool ghosts = false);\n",
    "void print_slab(const Matrix & m, int nrows, int ncols, bool ghosts);\n",
    "void send_rows(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void send_rows_non_blocking(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void send_rows_sendrecv(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void send_rows_topology(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void evolve(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  MPI_Init(&argc, &argv);\n",
    "  int pid, np;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
    "\n",
    "  const int N = std::atoi(argv[1]);\n",
    "  const double L = std::atof(argv[2]);\n",
    "  const int STEPS = std::atoi(argv[3]);\n",
    "  const double DELTA = L/N;\n",
    "\n",
    "  // problem partition\n",
    "  int NCOLS = N, NROWS = N/np + 2; // include ghosts\n",
    "  Matrix data(NROWS*NCOLS); // include ghosts cells\n",
    "  initial_conditions(data, NROWS, NCOLS, pid, np);\n",
    "  if (0 == pid) {std::cout << \" After initial conditions ...\\n\";}\n",
    "  print_screen(data, NROWS, NCOLS, pid, np); // todo\n",
    "\n",
    "  boundary_conditions(data, NROWS, NCOLS, pid, np); // todo\n",
    "  if (0 == pid) {std::cout << \" After boundary conditions ...\\n\";}\n",
    "  print_screen(data, NROWS, NCOLS, pid, np, true); // todo\n",
    "\n",
    "  //send_rows(data, NROWS, NCOLS, pid, np); // todo\n",
    "  //send_rows_non_blocking(data, NROWS, NCOLS, pid, np); // todo\n",
    "  //send_rows_sendrecv(data, NROWS, NCOLS, pid, np); // todo\n",
    "  send_rows_topology(data, NROWS, NCOLS, pid, np); // todo\n",
    "  if (0 == pid) {std::cout << \" After one comm ...\\n\";}\n",
    "  print_screen(data, NROWS, NCOLS, pid, np, true); // todo\n",
    "\n",
    "  if (0 == pid) {std::cout << \" After one relax step ...\\n\";}\n",
    "  evolve(data, NROWS, NCOLS, pid, np);\n",
    "  send_rows_topology(data, NROWS, NCOLS, pid, np); // todo\n",
    "  print_screen(data, NROWS, NCOLS, pid, np, true); // todo\n",
    "  if (0 == pid) {std::cout << \" After one relax step ...\\n\";}\n",
    "  evolve(data, NROWS, NCOLS, pid, np);\n",
    "  send_rows_topology(data, NROWS, NCOLS, pid, np); // todo\n",
    "  print_screen(data, NROWS, NCOLS, pid, np, true); // todo\n",
    "\n",
    "  /*\n",
    "  // Serial version\n",
    "  Matrix data(N*N);\n",
    "  initial_conditions(data, N, N, ...);\n",
    "  print_screen(...);\n",
    "  boundary_conditions(data, N, N, ...);\n",
    "  init_gnuplot();\n",
    "  for (int istep = 0; istep < STEPS; ++istep) {\n",
    "  evolve(data, N, N);\n",
    "  plot_gnuplot(data, DELTA, N, N);\n",
    "  }\n",
    "  */\n",
    "\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "/////////////////////////////////////////////////////\n",
    "// Parallel implementations\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // same task for all pids, but fill with the pids to distinguish among thems\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      m[ii*ncols + jj] = pid;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  <<boundary-student>>\n",
    "    }\n",
    "\n",
    "void print_screen(const Matrix & m, int nrows, int ncols, int pid, int np, bool ghosts)\n",
    "{\n",
    "  <<print-student>>\n",
    "    }\n",
    "\n",
    "void print_slab(const Matrix & m, int nrows, int ncols, bool ghosts)\n",
    "{\n",
    "  int imin = 0, imax = nrows;\n",
    "  if (false == ghosts) {\n",
    "    imin = 1; imax = nrows-1;\n",
    "  }\n",
    "  std::cout.precision(3);\n",
    "  for(int ii = imin; ii < imax; ++ii) {\n",
    "    for(int jj = 0; jj < ncols; ++jj) {\n",
    "      std::cout << std::setw(9) <<  m[ii*ncols + jj] << \" \";\n",
    "    }\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "  std::cout << \"\\n\";\n",
    "}\n",
    "\n",
    "void send_rows(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  <<send-rows-student>>\n",
    "    }\n",
    "\n",
    "void send_rows_non_blocking(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // send data forwards\n",
    "  MPI_Request r1;\n",
    "  if (pid <= np-2) {\n",
    "    //MPI_Send(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD);\n",
    "    MPI_Isend(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD, &r1);\n",
    "  }\n",
    "  if (pid >= 1) {\n",
    "    MPI_Recv(&m[0], ncols, MPI_DOUBLE, pid-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "  // send data backwards\n",
    "  MPI_Request r2;\n",
    "  if (pid >= 1) {\n",
    "    MPI_Isend(&m[(1)*ncols], ncols, MPI_DOUBLE, pid-1, 1, MPI_COMM_WORLD, &r2);\n",
    "  }\n",
    "  if (pid <= np-2) {\n",
    "    MPI_Recv(&m[(nrows-1)*ncols], ncols, MPI_DOUBLE, pid+1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "\n",
    "  if (pid <= np-2) {\n",
    "    MPI_Wait(&r1, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "\n",
    "  if (pid >= 1) {\n",
    "    MPI_Wait(&r2, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "}\n",
    "\n",
    "void send_rows_sendrecv(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  if (0 == pid) {\n",
    "    MPI_Sendrecv(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0,\n",
    "                 &m[(nrows-1)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "  if (1 <= pid && pid <= np-2) {\n",
    "    // send data forwards\n",
    "    MPI_Sendrecv(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0,\n",
    "                 &m[(0)*ncols], ncols, MPI_DOUBLE, pid-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    // send data backwards\n",
    "    MPI_Sendrecv(&m[(1)*ncols], ncols, MPI_DOUBLE, pid-1, 0,\n",
    "                 &m[(nrows-1)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);  }\n",
    "\n",
    "  if (np-1 == pid) {\n",
    "    MPI_Sendrecv(&m[(1)*ncols], ncols, MPI_DOUBLE, pid-1, 0,\n",
    "                 &m[(0)*ncols], ncols, MPI_DOUBLE, pid-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "}\n",
    "\n",
    "void send_rows_topology(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // create a cartesian 1D topology representing the non-periodic ring\n",
    "  // https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture28.pdf\n",
    "  // https://www.codingame.com/playgrounds/47058/have-fun-with-mpi-in-c/mpi-process-topologies\n",
    "  // MPI_Cart_create(MPI_Comm old_comm, int ndims, const int *dims, const int *periods, int reorder, MPI_Comm *comm_cart)\n",
    "  int ndims = 1, reorder = 1;\n",
    "  int dimsize[ndims] {np};\n",
    "  int periods[ndims] {0};\n",
    "  MPI_Comm comm;\n",
    "  MPI_Cart_create(MPI_COMM_WORLD, 1, &dimsize[0], &periods[0], reorder, &comm);\n",
    "\n",
    "  // Now use the network topology to communicate data in one pass for each direction\n",
    "  // https://www.open-mpi.org/doc/v3.0/man3/MPI_Neighbor_alltoallw.3.php\n",
    "  // https://www.events.prace-ri.eu/event/967/contributions/1110/attachments/1287/2213/MPI_virtual_topologies.pdf\n",
    "  MPI_Datatype types[2] = {MPI_DOUBLE, MPI_DOUBLE};\n",
    "  // NOTE: scounts[0] goes with rcounts[1] (left and right)\n",
    "  int scounts[2] = {ncols, ncols};\n",
    "  int rcounts[2] = {ncols, ncols};\n",
    "  MPI_Aint sdispl[2]{0}, rdispl[2] {0};\n",
    "  MPI_Get_address(&m[0]+(nrows-2)*ncols, &sdispl[1]); // send to next\n",
    "  MPI_Get_address(&m[0]+(0)*ncols, &rdispl[0]); // receive from previous\n",
    "  MPI_Get_address(&m[0]+(1)*ncols, &sdispl[0]); // send to previous\n",
    "  MPI_Get_address(&m[0]+(nrows-1)*ncols, &rdispl[1]); // receive from next\n",
    "  MPI_Neighbor_alltoallw(MPI_BOTTOM, scounts, sdispl, types,\n",
    "                         MPI_BOTTOM, rcounts, rdispl, types, comm);\n",
    "}\n",
    "\n",
    "void evolve(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  <<evolve-student>>\n",
    "    }\n",
    "\n",
    "\n",
    "/////////////////////////////////////////////////////\n",
    "// SERIAL VERSIONS\n",
    "\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      m[ii*ncols + jj] = 1.0;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  int ii = 0, jj = 0;\n",
    "\n",
    "  ii = 0;\n",
    "  for (jj = 0; jj < ncols; ++jj)\n",
    "    m[ii*ncols + jj] = 100;\n",
    "\n",
    "  ii = nrows-1;\n",
    "  for (jj = 0; jj < ncols; ++jj)\n",
    "    m[ii*ncols + jj] = 0;\n",
    "\n",
    "  jj = 0;\n",
    "  for (ii = 1; ii < nrows-1; ++ii)\n",
    "    m[ii*ncols + jj] = 0;\n",
    "\n",
    "  jj = ncols-1;\n",
    "  for (ii = 1; ii < nrows-1; ++ii)\n",
    "    m[ii*ncols + jj] = 0;\n",
    "}\n",
    "\n",
    "void evolve(Matrix & m, int nrows, int ncols)\n",
    "{\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      // check if boundary\n",
    "      if(ii == 0) continue;\n",
    "      if(ii == nrows-1) continue;\n",
    "      if(jj == 0) continue;\n",
    "      if(jj == ncols-1) continue;\n",
    "      // evolve non boundary\n",
    "      m[ii*ncols+jj] = (m[(ii+1)*ncols + jj] +\n",
    "                        m[(ii-1)*ncols + jj] +\n",
    "                        m[ii*ncols + jj + 1] +\n",
    "                        m[ii*ncols + jj - 1] )/4.0;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void print(const Matrix & m, double delta, int nrows, int ncols)\n",
    "{\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      std::cout << ii*delta << \" \" << jj*delta << \" \" <<  m[ii*ncols + jj] << \"\\n\";\n",
    "    }\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "}\n",
    "\n",
    "void init_gnuplot(void)\n",
    "{\n",
    "  std::cout << \"set contour \" << std::endl;\n",
    "  //std::cout << \"set terminal gif animate \" << std::endl;\n",
    "  //std::cout << \"set out 'anim.gif' \" << std::endl;\n",
    "}\n",
    "\n",
    "void plot_gnuplot(const Matrix & m, double delta, int nrows, int ncols)\n",
    "{\n",
    "  std::cout << \"splot '-' w pm3d \" << std::endl;\n",
    "  print(m, delta, nrows, ncols);\n",
    "  std::cout << \"e\" << std::endl;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Now we can just perform the relaxation step many times and create the\n",
    "animation after parallelizing gnuplot printing:\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <cmath>\n",
    "#include <iomanip>\n",
    "#include \"mpi.h\"\n",
    "\n",
    "typedef std::vector<double> Matrix; // alias\n",
    "\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols);\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols);\n",
    "void evolve(Matrix & m, int nrows, int ncols);\n",
    "void print(const Matrix & m, double delta, int nrows, int ncols);\n",
    "void init_gnuplot(void);\n",
    "void plot_gnuplot(const Matrix & m, double delta, int nrows, int ncols);\n",
    "\n",
    "// parallel versions\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void print_screen(const Matrix & m, int nrows, int ncols, int pid, int np, bool ghosts = false);\n",
    "void print_slab(const Matrix & m, int nrows, int ncols, bool ghosts);\n",
    "void send_rows(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void send_rows_non_blocking(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void send_rows_sendrecv(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void send_rows_topology(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void evolve(Matrix & m, int nrows, int ncols, int pid, int np);\n",
    "void init_gnuplot(int pid, int np);\n",
    "void plot_gnuplot(const Matrix & m, double delta, int nrows, int ncols, int pid, int np);\n",
    "void print_slab_gnuplot(const Matrix & m, double delta, int nrows, int ncols, int pid, int np);\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  MPI_Init(&argc, &argv);\n",
    "  int pid, np;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
    "\n",
    "  const int N = std::atoi(argv[1]);\n",
    "  const double L = std::atof(argv[2]);\n",
    "  const int STEPS = std::atoi(argv[3]);\n",
    "  const double DELTA = L/N;\n",
    "\n",
    "  // problem partition\n",
    "  int NCOLS = N, NROWS = N/np + 2; // include ghosts\n",
    "  Matrix data(NROWS*NCOLS); // include ghosts cells\n",
    "  initial_conditions(data, NROWS, NCOLS, pid, np);\n",
    "  //if (0 == pid) {std::cout << \" After initial conditions ...\\n\";}\n",
    "  //print_screen(data, NROWS, NCOLS, pid, np); // todo\n",
    "\n",
    "  boundary_conditions(data, NROWS, NCOLS, pid, np); // todo\n",
    "  //if (0 == pid) {std::cout << \" After boundary conditions ...\\n\";}\n",
    "  //print_screen(data, NROWS, NCOLS, pid, np, true); // todo\n",
    "\n",
    "\n",
    "  <<anim-student>>\n",
    "\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "/////////////////////////////////////////////////////\n",
    "// Parallel implementations\n",
    "void initial_conditions(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // same task for all pids, but fill with the pids to distinguish among thems\n",
    "  for(int ii=0; ii<nrows; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      m[ii*ncols + jj] = pid;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "void boundary_conditions(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  <<boundary-student>>\n",
    "}\n",
    "\n",
    "void print_screen(const Matrix & m, int nrows, int ncols, int pid, int np, bool ghosts)\n",
    "{\n",
    "  <<print-student>>\n",
    "}\n",
    "\n",
    "void print_slab(const Matrix & m, int nrows, int ncols, bool ghosts)\n",
    "{\n",
    "  int imin = 0, imax = nrows;\n",
    "  if (false == ghosts) {\n",
    "    imin = 1; imax = nrows-1;\n",
    "  }\n",
    "  std::cout.precision(3);\n",
    "  for(int ii = imin; ii < imax; ++ii) {\n",
    "    for(int jj = 0; jj < ncols; ++jj) {\n",
    "      std::cout << std::setw(9) <<  m[ii*ncols + jj] << \" \";\n",
    "    }\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "  std::cout << \"\\n\";\n",
    "}\n",
    "\n",
    "void send_rows(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  <<send-rows-student>>\n",
    "}\n",
    "\n",
    "void send_rows_non_blocking(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // send data forwards\n",
    "  MPI_Request r1;\n",
    "  if (pid <= np-2) {\n",
    "    //MPI_Send(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD);\n",
    "    MPI_Isend(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD, &r1);\n",
    "  }\n",
    "  if (pid >= 1) {\n",
    "    MPI_Recv(&m[0], ncols, MPI_DOUBLE, pid-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "  // send data backwards\n",
    "  MPI_Request r2;\n",
    "  if (pid >= 1) {\n",
    "    MPI_Isend(&m[(1)*ncols], ncols, MPI_DOUBLE, pid-1, 1, MPI_COMM_WORLD, &r2);\n",
    "  }\n",
    "  if (pid <= np-2) {\n",
    "    MPI_Recv(&m[(nrows-1)*ncols], ncols, MPI_DOUBLE, pid+1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "\n",
    "  if (pid <= np-2) {\n",
    "    MPI_Wait(&r1, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "\n",
    "  if (pid >= 1) {\n",
    "    MPI_Wait(&r2, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "}\n",
    "\n",
    "void send_rows_sendrecv(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  if (0 == pid) {\n",
    "    MPI_Sendrecv(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0,\n",
    "                 &m[(nrows-1)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "  if (1 <= pid && pid <= np-2) {\n",
    "  // send data forwards\n",
    "    MPI_Sendrecv(&m[(nrows-2)*ncols], ncols, MPI_DOUBLE, pid+1, 0,\n",
    "                 &m[(0)*ncols], ncols, MPI_DOUBLE, pid-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  // send data backwards\n",
    "    MPI_Sendrecv(&m[(1)*ncols], ncols, MPI_DOUBLE, pid-1, 0,\n",
    "                 &m[(nrows-1)*ncols], ncols, MPI_DOUBLE, pid+1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);  }\n",
    "\n",
    "  if (np-1 == pid) {\n",
    "    MPI_Sendrecv(&m[(1)*ncols], ncols, MPI_DOUBLE, pid-1, 0,\n",
    "                 &m[(0)*ncols], ncols, MPI_DOUBLE, pid-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  }\n",
    "}\n",
    "\n",
    "void send_rows_topology(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // create a cartesian 1D topology representing the non-periodic ring\n",
    "  // https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture28.pdf\n",
    "  // https://www.codingame.com/playgrounds/47058/have-fun-with-mpi-in-c/mpi-process-topologies\n",
    "  // MPI_Cart_create(MPI_Comm old_comm, int ndims, const int *dims, const int *periods, int reorder, MPI_Comm *comm_cart)\n",
    "  int ndims = 1, reorder = 1;\n",
    "  int dimsize[ndims] {np};\n",
    "  int periods[ndims] {0};\n",
    "  MPI_Comm comm;\n",
    "  MPI_Cart_create(MPI_COMM_WORLD, 1, &dimsize[0], &periods[0], reorder, &comm);\n",
    "\n",
    "  // Now use the network topology to communicate data in one pass for each direction\n",
    "  // https://www.open-mpi.org/doc/v3.0/man3/MPI_Neighbor_alltoallw.3.php\n",
    "  // https://www.events.prace-ri.eu/event/967/contributions/1110/attachments/1287/2213/MPI_virtual_topologies.pdf\n",
    "  MPI_Datatype types[2] = {MPI_DOUBLE, MPI_DOUBLE};\n",
    "  // NOTE: scounts[0] goes with rcounts[1] (left and right)\n",
    "  int scounts[2] = {ncols, ncols};\n",
    "  int rcounts[2] = {ncols, ncols};\n",
    "  MPI_Aint sdispl[2]{0}, rdispl[2] {0};\n",
    "  MPI_Get_address(&m[0]+(nrows-2)*ncols, &sdispl[1]); // send to next\n",
    "  MPI_Get_address(&m[0]+(0)*ncols, &rdispl[0]); // receive from previous\n",
    "  MPI_Get_address(&m[0]+(1)*ncols, &sdispl[0]); // send to previous\n",
    "  MPI_Get_address(&m[0]+(nrows-1)*ncols, &rdispl[1]); // receive from next\n",
    "  MPI_Neighbor_alltoallw(MPI_BOTTOM, scounts, sdispl, types,\n",
    "                         MPI_BOTTOM, rcounts, rdispl, types, comm);\n",
    "}\n",
    "\n",
    "void evolve(Matrix & m, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  <<evolve-student>>\n",
    "}\n",
    "\n",
    "void init_gnuplot(int pid, int np)\n",
    "{\n",
    "  if (0 == pid) {\n",
    "    std::cout << \"set contour \" << std::endl;\n",
    "    std::cout << \"set terminal gif animate \" << std::endl;\n",
    "    std::cout << \"set out 'anim.gif' \" << std::endl;\n",
    "  }\n",
    "}\n",
    "\n",
    "void plot_gnuplot(const Matrix & m, double delta, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  if (0 == pid) {\n",
    "    std::cout << \"splot '-' w pm3d \" << std::endl;\n",
    "    // print master data\n",
    "    print_slab_gnuplot(m, delta, nrows, ncols, pid, np);\n",
    "    // now receive and print other pdis data\n",
    "    Matrix buffer(nrows*ncols);\n",
    "    for (int ipid = 1; ipid < np; ++ipid) {\n",
    "      MPI_Recv(&buffer[0], nrows*ncols, MPI_DOUBLE, ipid, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "      print_slab_gnuplot(buffer, delta, nrows, ncols, pid, np);\n",
    "    }\n",
    "    std::cout << \"e\" << std::endl;\n",
    "  } else { // workers send\n",
    "    MPI_Send(&m[0], nrows*ncols, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n",
    "  }\n",
    "}\n",
    "\n",
    "void print_slab_gnuplot(const Matrix & m, double delta, int nrows, int ncols, int pid, int np)\n",
    "{\n",
    "  // needs to fix local index with global ones\n",
    "  // ignore ghosts\n",
    "  for(int ii=1; ii<nrows-1; ++ii) {\n",
    "    for(int jj=0; jj<ncols; ++jj) {\n",
    "      std::cout << (ii-1 + nrows*pid)*delta << \" \" << (jj-1)*delta << \" \" <<  m[ii*ncols + jj] << \"\\n\";\n",
    "    }\n",
    "    std::cout << \"\\n\";\n",
    "  }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822e20b",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercises\n",
    "\n",
    "Parallelize the code used on the relaxation method to solve the Poisson\n",
    "equation, including some charge densities and some ineer boundary\n",
    "conditions. Here you will need to decompose the domain, keep some ghost\n",
    "zones, and share some info across processes to update the ghost zones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e48e6",
   "metadata": {},
   "source": [
    "\n",
    "## Debugging in parallel\n",
    "\n",
    "-   <https://www.open-mpi.org/faq/?category=debugging>\n",
    "-   <https://github.com/Azrael3000/tmpi>\n",
    "-   <https://stackoverflow.com/questions/329259/how-do-i-debug-an-mpi-program>\n",
    "-   <https://hpc.llnl.gov/software/development-environment-software/stat-stack-trace-analysis-tool>\n",
    "\n",
    "wget\n",
    "<https://epcced.github.io/2022-04-19-archer2-intro-develop/files/gdb4hpc_exercise.c>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9312e37",
   "metadata": {},
   "source": [
    "\n",
    "## More Exercises\n",
    "\n",
    "1.  Check the exercises on\n",
    "    <https://computing.llnl.gov/tutorials/mpi/#Exercise1>,\n",
    "    <https://computing.llnl.gov/tutorials/mpi/#Exercise2>, and\n",
    "    <https://computing.llnl.gov/tutorials/mpi/#Exercise3> .\n",
    "2.  Check <https://www.nics.tennessee.edu/mpi-tutorial>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
