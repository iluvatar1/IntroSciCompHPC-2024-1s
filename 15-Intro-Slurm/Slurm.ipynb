{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd5fb3b-212f-44d7-bed0-60d0e8e2c136",
   "metadata": {},
   "source": [
    "# HPC resource manager: Slurm\n",
    "\n",
    "Slurm, <https://slurm.schedmd.com/documentation.html>, is a resource\n",
    "manager and job scheduler that allows to organize and share resources in\n",
    "a HPC environment to get an optimal usage. It allows to specify usage\n",
    "policies, limits in terms of memory or cpu etc, to get metrics regarding\n",
    "the cluster usage, and so on. You can watch a good intro at\n",
    "<https://www.youtube.com/watch?v=NH_Fb7X6Db0> and related videos.\n",
    "\n",
    "Here we will use a very simple installation in the computer room. Our\n",
    "goal will be to learn some of the basic commands to get the possible\n",
    "resources, how to specify a partition, limits, resources and so on.\n",
    "\n",
    "First of all, log into a client and use the command `sinfo` to get\n",
    "information about partitions:\n",
    "\n",
    "``` shell\n",
    "sinfo --all\n",
    "```\n",
    "\n",
    "As you can see in this example, there are several partitions available\n",
    "to be used. The `2threads` partition is the default. Some nodes\n",
    "(`sala31`) are not working. There is not time limit besides the login\n",
    "node (which actually should not be used for any job). Use the manual and\n",
    "get some other info about the resources.\n",
    "\n",
    "Now let's run some simple commands in the cluster. To do so, we will use\n",
    "the simple command `srun` (check the manual)\n",
    "\n",
    "``` shell\n",
    "srun hostname\n",
    "```\n",
    "\n",
    "As you can see here, the command actually ran in the `2threads`\n",
    "partition since we did not specify the actual partitions and `2threads`\n",
    "is the default.\n",
    "\n",
    "**Exercise**: Run 18 instances of the same command in a non-default\n",
    "partition. You should get something like (`12threads` partition)\n",
    "\n",
    "|        |\n",
    "|--------|\n",
    "| sala38 |\n",
    "| sala37 |\n",
    "| sala38 |\n",
    "| sala37 |\n",
    "| sala38 |\n",
    "| sala38 |\n",
    "| sala38 |\n",
    "| sala38 |\n",
    "| sala37 |\n",
    "| sala37 |\n",
    "| sala37 |\n",
    "| sala37 |\n",
    "| sala38 |\n",
    "| sala37 |\n",
    "| sala37 |\n",
    "| sala37 |\n",
    "| sala37 |\n",
    "| sala38 |\n",
    "\n",
    "As you can see, the jobs where magically distributed among two nodes\n",
    "necessary to run 18 processes (each node allows for 12 processes, two\n",
    "nodes corresponds to 24 processes in total). If you want to see this\n",
    "better, use the stress command with a timeout of 10 seconds and check\n",
    "that as soon as you launch the process, two nodes will be using their\n",
    "cpus at full (have some `htop` command running on both nodes):\n",
    "\n",
    "``` shell\n",
    "srun -p 12threads -n 18  stress -t 10 -c 1\n",
    "```\n",
    "\n",
    "This is very useful, you can just distribute your commands among all the\n",
    "computers belonging to a given partition. But in general it is much\n",
    "better to write this commands in a script that could be reused.\n",
    "Actually, you can employ a special syntaxt in your script to give all\n",
    "the info to slurm and then use the command `sbatch` to launch your\n",
    "script , and `squeue` to check its state. You can use a script generator\n",
    "to make this task easier:\n",
    "\n",
    "-   <https://wiki.nlhpc.cl/Generador_Scripts>\n",
    "-   <https://www.hpc.iastate.edu/guides/classroom-hpc-cluster/slurm-job-script-generator>\n",
    "-   <https://hpc.nmsu.edu/home/tools/slurm-script-generator/>\n",
    "-   <https://www-app.igb.illinois.edu/tools/slurm/>\n",
    "-   <https://user.cscs.ch/access/running/jobscript_generator/>\n",
    "-   â€¦\n",
    "\n",
    "For our example we will need to generate and adapt to finally get\n",
    "something like\n",
    "\n",
    "``` shell\n",
    "#!/bin/bash -l\n",
    "#SBATCH --job-name=\"testmulti\"\n",
    "# #SBATCH --account=\"HerrComp\" # not used\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=wfoquendop@unal.edu.co\n",
    "#SBATCH --time=01:00:00\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=12\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --partition=12threads\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "srun hostname\n",
    "```\n",
    "\n",
    "and then run it as\n",
    "\n",
    "``` shell\n",
    "sbatch run.sh\n",
    "```\n",
    "\n",
    "You can get info about the jobs (if it is running, pending, cancelled,\n",
    "etc) using the command `squeue` .\n",
    "\n",
    "By default you will get the output written in some `*.out` file\n",
    "\n",
    "|              |\n",
    "|--------------|\n",
    "| Slurm.org    |\n",
    "| run.sh       |\n",
    "| slurm-70.out |\n",
    "\n",
    "Using a slurm script allows for a very general way to both run commands\n",
    "and specifiy , for instance, what modules to load, like using\n",
    "`ml somehpclib` or `spack load something`.\n",
    "\n",
    "**Exercise:** Create a script to run the stress command in some\n",
    "partition, including several nodes. Log into those nodes and check the\n",
    "actual usage. Share with the other students.\n",
    "\n",
    "## Example: Running the laplace mpi simulation using slurm\n",
    "\n",
    "Here we will use at least two nodes to run the laplace simulation. Of\n",
    "course the performance will be affected since our network is slow, but\n",
    "the goal here is to be able to run the command.\n",
    "\n",
    "``` shell\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "#SBATCH --job-name=\"testmulti\"\n",
    "# #SBATCH --account=\"HerrComp\" # not used\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=wfoquendop@unal.edu.co\n",
    "#SBATCH --time=01:00:00\n",
    "#SBATCH --nodes=3\n",
    "#SBATCH --ntasks-per-node=6\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --partition=12threads\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "cd $HOME/repos/2021-II-HerrComp/2022-02-09-Slurm/\n",
    "mpic++ MPI_laplace_v5.cpp\n",
    "date\n",
    "srun --mpi=pmi2 ./a.out 180 1.4705 200 | gnuplot\n",
    "date\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
